{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"<p>Joshua Ewer's Data Science Portfolio</p> <p>Welcome to a few year's worth of hard work learning how to using machine learning and data science methodologies to solve hard problems!  This portfolio contains ten examples of different exploratory data methods, machine learning models, and various ways to tell a story with data.</p> <ul> <li>Projects will get you to a detailed list of each project.</li> <li>Concepts groups each of these projects into common tags</li> <li>About will help you contact me with any questions, comments, or networking.</li> </ul> <p>Enjoy!</p>"},{"location":"concepts/","title":"Projects Grouped by Demonstrated Concepts","text":""},{"location":"concepts/#tag:clustering","title":"Clustering","text":"<ul> <li>            Retail Customer Segmentation          </li> </ul>"},{"location":"concepts/#tag:convolutional-neural-network","title":"Convolutional Neural Network","text":"<ul> <li>            Optical Recognition with Tensorflow/Keras          </li> </ul>"},{"location":"concepts/#tag:data-visualization","title":"Data Visualization","text":"<ul> <li>            Childcare Costs          </li> <li>            Using Collaborative Filtering to Build a Recommender          </li> </ul>"},{"location":"concepts/#tag:exploratory-data-analysis","title":"Exploratory Data Analysis","text":"<ul> <li>            Childcare Costs          </li> <li>            Predicting Life Expectancy Using Global Health Data          </li> <li>            Predicting Presence of Heart Disease          </li> <li>            Retail Customer Segmentation          </li> <li>            Using Collaborative Filtering to Build a Recommender          </li> </ul>"},{"location":"concepts/#tag:hyperparameter-tuning","title":"Hyperparameter Tuning","text":"<ul> <li>            GridSearch for hyperparameter tuning          </li> <li>            Time-series Retail Analysis          </li> </ul>"},{"location":"concepts/#tag:keras","title":"Keras","text":"<ul> <li>            Optical Recognition with Tensorflow/Keras          </li> </ul>"},{"location":"concepts/#tag:machine-learning","title":"Machine Learning","text":"<ul> <li>            Predicting Life Expectancy Using Global Health Data          </li> <li>            Predicting Presence of Heart Disease          </li> <li>            Retail Customer Segmentation          </li> <li>            Time-series Retail Analysis          </li> </ul>"},{"location":"concepts/#tag:presentation","title":"Presentation","text":"<ul> <li>            Childcare Costs          </li> <li>            Using Collaborative Filtering to Build a Recommender          </li> </ul>"},{"location":"concepts/#tag:regression","title":"Regression","text":"<ul> <li>            Predicting Life Expectancy Using Global Health Data          </li> <li>            Predicting Presence of Heart Disease          </li> <li>            Retail Customer Segmentation          </li> </ul>"},{"location":"concepts/#tag:tensorflow","title":"Tensorflow","text":"<ul> <li>            Optical Recognition with Tensorflow/Keras          </li> </ul>"},{"location":"about/about/","title":"About Me","text":"<p>I'm a professional and committed software engineer with over 25 years of experience in a wide range of industries, platforms and corporate environments.</p> <p>As a technology leader, I work with business and management representatives to gather requirements, plan timelines and budgets, and manage teams' technical delivery. I really enjoy working with a development team and helping them to refine and improve their process through the (gradual and team-appropriate) adoption of Agile principals and techniques.</p> <p>As a software architect, I design and implement systems so I can deliver working products to customers. Despite enjoying the \"people\" aspects of my job, I never forget that a deep and broad technical skillset is my most important attribute. I love standing in front of a whiteboard and drawing boxes-and-lines, then turning around and coding the night away.</p> <p>As a data engineer, I dig into complex, large datasets and surface interesting data for further analysis and build products to show the value of the data. Whether it's moving TB of data across cloud providers, building and automating predictive models, or visualizing data in interactive charts or infographs,, it's always exciting to find something new and interesting in data you didn't expect to see.</p> <p>For more detail, see my linkedin account at:  https://www.linkedin.com/in/joshuaewer/</p>"},{"location":"about/contact/","title":"Contact","text":"<p>You can contact me in a couple of different ways.  I'm always open to talking to recruiters and helping to connect the right people with the right companies and positions.</p> <ul> <li>LinkedIn</li> <li>Email</li> </ul>"},{"location":"projects/childcare/","title":"Childcare Costs","text":"<p>This project analyzes U.S. childcare cost data from 2008\u20132018 to show how childcare\u2014especially infant care\u2014has become more expensive over time and varies widely by state and county. Using clear visualizations and a data storytelling approach, it highlights how rising childcare costs create a growing financial burden for families and frames the issue as a policy problem rather than an individual one.</p>","tags":["Exploratory Data Analysis","Data Visualization","Presentation"]},{"location":"projects/childcare/#working-code","title":"Working Code","text":"<ul> <li>Whitepaper here: ChildcareCosts.pdf</li> <li>Powerpoint here: ChildcareCosts.pptx</li> <li>Notebook here: ChildcareCosts.ipynb</li> </ul>","tags":["Exploratory Data Analysis","Data Visualization","Presentation"]},{"location":"projects/gridsearch/","title":"GridSearch for hyperparameter tuning","text":"<p>This notebook walks through building and tuning a few different machine learning models to predict loan outcomes. After loading and cleaning the loan training data (including dropping IDs and preparing features), the notebook tests several common classification approaches: K-Nearest Neighbors, Logistic Regression, and Random Forest. Instead of guessing hyperparameters, it uses GridSearchCV to systematically try different settings for each model and compare their performance using cross-validation.</p> <p>In the end, Logistic Regression comes out on top. The grid search identifies an optimal regularization strength (C) that balances model complexity without overfitting, and the liblinear solver is used since it\u2019s well-suited for binary classification problems like this one. Overall, the notebook demonstrates a practical, methodical approach to model selection\u2014trying multiple algorithms, tuning them properly, and letting the data (not intuition alone) decide which model performs best.</p>","tags":["Hyperparameter Tuning"]},{"location":"projects/gridsearch/#working-code","title":"Working Code","text":"<ul> <li>Notebook here: gridsearch.ipynb</li> </ul>","tags":["Hyperparameter Tuning"]},{"location":"projects/heart/","title":"Predicting Presence of Heart Disease","text":"<p>This project looks at whether everyday clinical measurements can be used to help identify patients who may have heart disease. Using a public dataset of just anonymous patient records, it explores how information like chest pain type, blood pressure, heart rate, and stress test results can be combined to support clinical decision-making, with the goal of highlighting higher-risk cases rather than replacing medical judgment.</p> <p>The project compared a simple, easy-to-interpret Logistic Regression model with a more advanced machine learning approach (Gradient Boosting ensemble) to see how much predictive performance could be improved. The more flexible model performed significantly better, showing that patterns in routine health data can be used to make surprisingly accurate predictions, while also reinforcing the importance of using these tools carefully and ethically in healthcare settings.</p>","tags":["Machine Learning","Exploratory Data Analysis","Regression"]},{"location":"projects/heart/#working-code","title":"Working Code","text":"<ul> <li>Whitepaper here: HeartDisease.pdf</li> <li>Powerpoint here: HeartDisease.pptx</li> <li>Notebook here: HeartDisease.ipynb</li> </ul>","tags":["Machine Learning","Exploratory Data Analysis","Regression"]},{"location":"projects/keras/","title":"Optical Recognition with Tensorflow/Keras","text":"<p>This notebook focuses on building a simple convolutional neural network (CNN) using Keras to classify handwritten digits from the MNIST dataset. It starts by loading and visualizing a few sample images to get a feel for the data, then preprocesses the images by reshaping them into the format expected by a CNN and normalizing the pixel values. The labels are one-hot encoded so they work cleanly with a multiclass neural network setup.</p> <p>From there, the notebook defines and trains a CNN with convolutional, pooling, and dense layers to learn visual patterns in the digits. After training, the model is evaluated on the test set, and its predictions are analyzed using a confusion matrix to see where it performs well and where it struggles. Overall, the notebook demonstrates a straightforward end-to-end deep learning workflow: loading image data, building a neural network, training it, and interpreting the results visually rather than relying on accuracy alone.</p>","tags":["Tensorflow","Keras","Convolutional Neural Network"]},{"location":"projects/keras/#working-code","title":"Working Code","text":"<ul> <li>Notebook here: opticalkeras.ipynb</li> </ul>","tags":["Tensorflow","Keras","Convolutional Neural Network"]},{"location":"projects/life/","title":"Predicting Life Expectancy Using Global Health Data","text":"<p>This project looks at how health, economic, and social factors are connected to life expectancy across countries using publicly available data from the World Health Organization. After cleaning the data and filling in missing values, the analysis explored patterns in variables like vaccination rates, infant mortality, healthcare spending, GDP, and poverty. Several models were then used to predict life expectancy, starting with linear regression and followed by more flexible machine learning approaches like Random Forest and XGBoost, with cross-validation used to compare performance.</p> <p>The results showed that life expectancy can be predicted quite accurately using a mix of health and socioeconomic indicators, with Random Forest performing slightly better while still avoiding overfitting. Many of the findings matched common expectations, such as lower life expectancy being linked to higher poverty and infant mortality, and higher life expectancy being associated with strong vaccination coverage. Overall, the project highlights how data science techniques can be used to better understand global health patterns, while also acknowledging data limitations and the importance of careful, non-causal interpretation.</p>","tags":["Exploratory Data Analysis","Machine Learning","Regression"]},{"location":"projects/life/#working-code","title":"Working Code","text":"<ul> <li>Whitepaper here: who_life.pdf</li> <li>Notebook here: who_life.ipynb</li> </ul>","tags":["Exploratory Data Analysis","Machine Learning","Regression"]},{"location":"projects/list/","title":"Joshua Ewer's Data Science Portfolio","text":""},{"location":"projects/list/#project-descriptions","title":"Project Descriptions","text":""},{"location":"projects/list/#retail-customer-segmentation","title":"Retail Customer Segmentation","text":"<p>Using supervised learning (Ridge, MLP) to forecast inventory demand, and unsupervised learning (K-means, RFM) to segment customers into meaningful groups. </p> <p>For more detail, click here</p>"},{"location":"projects/list/#predicting-presence-of-heart-disease-through-clinical-results","title":"Predicting Presence of Heart Disease Through Clinical Results","text":"<p>This project evaluates whether routinely collected clinical data can be used to predict the presence of heart disease, using a public dataset anonymous patient records.  It demonstrates careful data preparation, feature selection, and ethical considerations around false negatives in healthcare. </p> <p>This project conmpares a Logistic Regression baseline to a Gradient Boosting model (with significantly higher performance), showing how non-linear models can improve predictive accuracy.</p> <p>For more detail, click here</p>"},{"location":"projects/list/#life-expectancy-analysis-and-predictions","title":"Life Expectancy Analysis and Predictions","text":"<p>This project analyzes World Health Organization life expectancy data to identify which health, economic, and social factors are most strongly associated with differences in life expectancy across countries. Using exploratory analysis and predictive models including linear regression, Random Forest, and XGBoost, the work emphasizes interpretability, ethical use of global health data, and insights that can support evidence-based public health decision-making.</p> <p>For more detail, click here</p>"},{"location":"projects/list/#time-series-retail-analysis","title":"Time-series Retail Analysis","text":"<p>This notebook demonstrates time series forecasting workflow using real-world U.S. retail sales data.  It compares SARIMA and Holt-Winters algorithms to identify long-term trends and seasonal patterns. </p> <p>For more detail, click here</p>"},{"location":"projects/list/#childcare-costs","title":"Childcare Costs","text":"<p>This project analyzes U.S. childcare cost data from 2008\u20132018 to show how childcare\u2014especially infant care\u2014has become more expensive over time and varies widely by state and county. Using clear visualizations and a data storytelling approach, it highlights how rising childcare costs create a growing financial burden for families and frames the issue as a policy problem rather than an individual one.</p> <p>For more detail, click here</p>"},{"location":"projects/list/#optical-recognition-with-tensorflowkeras","title":"Optical Recognition with Tensorflow/Keras","text":"<p>Using keras and tensorflow, demonstrate using optical recognition to build a model that can identify a digit from a handwritten sample.</p> <p>For more detail, click here</p>"},{"location":"projects/list/#gridsearch-for-hyperparameter-tuning","title":"GridSearch for hyperparameter tuning","text":"<p>Gridsearch is a helpful method for finding the best settings (hyperparameters) for a machine learning model.  This notebook demonstrates using gridsearch to find the best hyperparameters for a selection of algorithms.</p> <p>For more detail, click here</p>"},{"location":"projects/list/#reducing-dimensionality-with-pca","title":"Reducing Dimensionality with PCA","text":"<p>When you have a dataset with a high numnber of variables that overlap or are highly correlated, Principal Component Analysis (PCA) is a useful technique to reduce the number of features in a dataset, without removing useful information.  This notebook demonstrates using PCA in a dataset of house sales to reduce the number of features without impacting the quality of the trained model.</p> <p>For more detail, click here</p>"},{"location":"projects/list/#comparing-regression-algorithms","title":"Comparing Regression Algorithms","text":"<p>There are a large number of regression algorithms available when trying to build a predictive model.  This notebook demonstrates comparing a selection of regression algorithms.  It also demonstrates using R^2 and RSME to calculate the performance of a model. </p> <p>For more detail, click here</p>"},{"location":"projects/list/#using-collaborative-filtering-to-build-a-recommender","title":"Using Collaborative Filtering to Build a Recommender","text":"<p>Collaborative filtering is a technique used in recommender systems that makes predictions about a user\u2019s interests by analyzing patterns of behavior or preferences from many users. This notebook demonstrates building a recommender for movies using cosine similarity based on a centered rating from all users. </p> <p>For more detail, click here</p>"},{"location":"projects/pca/","title":"Pca","text":"<p>tags:   - PCA   - Machine Learning</p>"},{"location":"projects/pca/#reducing-dimensionality-with-pca","title":"Reducing Dimensionality with PCA","text":"<p>This notebook walks through a couple of common feature reduction and selection techniques using real datasets, with a focus on how they affect model performance. It uses the Ames housing dataset to explore dimensionality reduction in a regression context. After cleaning the data (handling missing values and one-hot encoding categorical features), a baseline linear regression model is trained to predict house prices. From there, the notebook applies PCA to retain 90% of the variance and compares the resulting model\u2019s performance to the original. It also experiments with variance thresholding after scaling the features, showing how removing low-variance predictors impacts model accuracy and error.</p>"},{"location":"projects/pca/#working-code","title":"Working Code","text":"<ul> <li>Notebook here: pca.ipynb</li> </ul>"},{"location":"projects/recommender/","title":"Using Collaborative Filtering to Build a Recommender","text":"<p>This notebook builds a simple movie recommender system step by step, starting with some light exploratory analysis to understand the ratings data. It begins with a baseline popularity-based recommender, which just suggests the most commonly or highly rated movies overall. This approach is straightforward and works as a reliable fallback, even though it doesn\u2019t personalize recommendations. It\u2019s used both as a benchmark and as a safety net later on if a user requests recommendations for a movie that isn\u2019t found in the dataset.</p> <p>From there, the notebook moves into collaborative filtering using cosine similarity on user rating vectors. Ratings are mean-centered to reduce user bias (since some people rate everything high and others rate everything low), and similarities are calculated based on shared rating patterns. The idea is to find users with similar tastes (\u201cneighbors\u201d) and recommend movies they liked that the target user hasn\u2019t seen yet. The notebook also includes practical helper functions to make the system more usable, like a movie title lookup that tries exact matches, substrings, and fuzzy matching to handle typos or partial titles. Overall, the notebook focuses on building an intuitive, behavior-based recommender while balancing simplicity, robustness, and real-world usability.</p>","tags":["Exploratory Data Analysis","Data Visualization","Presentation"]},{"location":"projects/recommender/#working-code","title":"Working Code","text":"<ul> <li>Notebook here: recommender.ipynb</li> </ul>","tags":["Exploratory Data Analysis","Data Visualization","Presentation"]},{"location":"projects/regression/","title":"Regression","text":"<p>tags:   - Regression   - Machine Learning</p>"},{"location":"projects/regression/#comparing-regression-algorithms","title":"Comparing Regression Algorithms","text":"<p>This notebook walks through a straightforward regression workflow using the classic auto MPG dataset, focusing on predicting fuel efficiency from vehicle characteristics. It starts by loading and cleaning the data, then explores relationships between variables using correlation analysis and visualizations. Several features like displacement, cylinders, horsepower, and weight are shown to be highly correlated with MPG, with a clear negative linear relationship between vehicle weight and fuel efficiency. These exploratory steps help motivate the modeling choices later on.</p> <p>From there, the notebook builds and evaluates multiple regression models. A basic linear regression is trained using a train/test split, and performance is measured with R\u00b2, RMSE, and MAE, showing that the model explains roughly 70% of the variance in MPG with reasonably low error. A decision tree regressor is then tested, which performs extremely well on the training data but poorly on the test set \u2014 a clear example of overfitting. To address this, the notebook finishes by experimenting with ridge regression and cross-validation, showing how regularization and better validation strategies can improve generalization. Overall, the notebook does a nice job demonstrating not just how to build regression models, but how to diagnose and respond to common modeling pitfalls.</p>"},{"location":"projects/regression/#working-code","title":"Working Code","text":"<ul> <li>Notebook here: regression.ipynb</li> </ul>"},{"location":"projects/retail/","title":"Retail Customer Segmentation","text":"<p>This project shows how data science can be used to make better business decisions. </p> <p>The main goals are to forecast sales demand and to group customers based on how they shop. The analysis uses a simulated retail dataset from Kaggle that includes basic transaction details like dates, product categories, quantities, and prices. The focus of the project is less about perfect predictions and more about demonstrating a practical workflow for exploring data, creating useful features, and applying machine learning in a retail context.</p> <p>To do this, the project uses a mix of supervised and unsupervised learning techniques. Sales forecasting is handled as a regression problem, starting with Ridge regression as a stable baseline and then using a neural network to capture more complex patterns like seasonality and nonlinear relationships. Customer segmentation is done with K-means clustering using Recency, Frequency, and Monetary (RFM) features to identify different types of shoppers. </p> <p>Even though the data is simulated, the results show realistic trends and customer behaviors, making this a solid example of how predictive analytics can support inventory planning, marketing, and customer retention.</p>","tags":["Machine Learning","Exploratory Data Analysis","Regression","Clustering"]},{"location":"projects/retail/#working-code","title":"Working Code","text":"<ul> <li>Whitepaper here: RetailCustomerAnalysis.pdf</li> <li>Notebook here: Retail notebook</li> </ul>","tags":["Machine Learning","Exploratory Data Analysis","Regression","Clustering"]},{"location":"projects/sarima/sarima/","title":"Time-series Retail Analysis","text":"","tags":["Machine Learning","Hyperparameter Tuning"]},{"location":"projects/sarima/sarima/#background","title":"Background","text":"<p>This notebook demonstrates a complete time series forecasting workflow using real-world U.S. retail sales data. Key techniques include reshaping wide-format data into a tidy time series structure, constructing a proper datetime index, and performing exploratory visualization to identify long-term trends and seasonal patterns. </p> <p>The analysis applies automated SARIMA modeling for seasonal forecasting, including train/test splitting, out-of-sample prediction, and quantitative evaluation using RMSE. To contextualize model performance, the notebook also implements a Holt\u2013Winters exponential smoothing model as a comparative baseline. </p> <p>Together, these steps highlight practical skills in data preparation, seasonal time series modeling, model evaluation, and visual interpretation using Python\u2019s pandas, statsmodels, and matplotlib libraries.</p>","tags":["Machine Learning","Hyperparameter Tuning"]},{"location":"projects/sarima/sarima/#sample-visualizations","title":"Sample Visualizations","text":"","tags":["Machine Learning","Hyperparameter Tuning"]},{"location":"projects/sarima/sarima/#working-code","title":"Working Code","text":"<p>To see a working example, click here.</p>","tags":["Machine Learning","Hyperparameter Tuning"]}]}